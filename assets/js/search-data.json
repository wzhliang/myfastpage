{
  
    
        "post0": {
            "title": "On Kubernetes",
            "content": "General . backed by google and has a long history (in container’s term) | currently managed by Linux foundation | google also provide GKE (Google Container Engine) built on top of it | supports both docker and rtk | bad UI | container-agnostic orchestration system docker is one of the supported run-time | and currently the only one officially | . | documentation is sub par but Slack and Stackoverflow community is very active | apache license kubernetes/LICENSE at master · kubernetes/kubernetes · GitHub | . | GFW, Aliyun repo: registry.cn-hangzhou.aliyuncs.com/google_containers | quarterly release cycle | . History . originated from Google internal project Borg (now called Omega) | open sourced in 2014 | 1.0: July 2015 | graduated from CNCF (first project to do so) in March 2018 | . Terminologies . node a worker machine in k8s | also called minion | . | pod a smallest deployable unit | logical collection of containers that belong to an application | containers that should go hand in hand together as if they were in a single physical host | has single shared unique dynamic virtual IP | shared volume | most of the time it has a single container | think of it as a way of running multiple processes with container | share -net=container:bla | -ipc=container:bla | volume | PID name space | time namespace | . | there is a pause container that holds the network namespace for all other containers that has separate life cycle so docker - What work does the process in container &quot;gcr.io/google_containers/pause:0.8.0&quot; do? - Stack Overflow | . | . | . | replica set in terms of pods | k8s maintain user specified number of pods in a set | kill or add pod when necessary | replaces replication controller | . | replication controller takes care of one or more pods | make sure of the correct number of pods are running | this is an k8s object that’s injected into the cluster, not a daemon like kubelet | . | deployments declarative update for pods/replica set | describe only desired state | image, cpu, mem, envar | it’s like app configuration | uses replication set | deployment file are pretty much like docker compose file | Deployment -&gt; Replica Set -&gt; Pod(s) RS gets automatically created | . | apiVersion: extensions/v1beta1 | kind: Deployment | higher level concepts than replication set | Deployment.spec.revisionHistoryLimit: 100 asks to store 100 history default is 10 | . | . | service abstraction on top of pod | provides single static virtual IP and DNS name | provides load balancing | a named load balancer | can be exposed both externally and internally to the cluster | can also expose non-k8s endpoints | . | label key/value pair attached to an object, such as pod | used to group pods, etc | values are restricted to [a-zA-Z0-9]-_ annotation doesn’t have such restriction | . | . | selector the other side of label | can be used to search for pod, etc | . | namespace total separation of environments | e.g staging, production | . | kubernetes has an implicit assumption that all the state that is shared between service instances (e.g. a Mongo cluster that stores user profiles) is managed outside of Kubernetes. from here | . | . Components . Cluster Components . master manages a number of nodes | HA is available | runs API server, scheduler, and controllers | scheduler decides where a pod goes into the cluster | essentially it’s just a node running master processes | typically doesn’t run pods | . | node also called minion | manages pods, volume, secretes, etc | health check | proxy for port forwarding, etc | runs kubelet, kube-proxy and things like fluentd | also runs optional addons like dns, dashboard UI | . | . Software Components . etcd lightweight key-value data store | distributed database | can be configured to form a cluster automatically | . | API Server serves kubernets API over HTTP+JSON | kubernetes API | extension API | autoscaling API | batch API | . | scheduler monitors resource and decide which pod runs on which node | . | controller manager makes sure the current status matches user’s desire | . | kubelet runs on nodes, drives pod stat towards desired one | . | kube-proxy Communication between pods in Kubernetes is managed by the Service resource. By default, this resource creates a virtual IP address: the ClusterIP. When a pod decides it wants to talk to another service, DNS returns the cluster IP of this service. As the pod tries to connect to the cluster IP, iptables on the local node has been configured to pick a destination pod IP address randomly. kube-proxy is in charge of configuring iptables on each node in the cluster and does this whenever a service is changed, a pod starts or stops. These changes happen on every node and because of iptables implementation details are extremely expensive. here | . | forward network traffic looking for cluster IP service to their endpoint | runs on every node | network proxy and load-balancer | monitors API from master | uses virtual IP and iptables | at one stage it was a real proxy that packets pass through | userspace mode -&gt; iptable -&gt; moved on to use ipvs | debug kube proxy with iptable rule examples | KUBE-SVC-* rule and KUBE-SEP-* rule | . | k8s blog, 2019, bug a war story about a bug, with clear explanation of iptables programmed by kube-proxy | KUBE-SERVICES - entrypoin, points to KUBE-SVC chain | KUBE-SVC-* chain - one per clusterIP service, load balanced to KUBE-SEP-* chain | KUBE-SEP-* chain - one per service endpoint, simple DNAT | . | prefetch with actual iptable chain examples | . | ipvs, k8s.io, 2018 iptable is a bottle neck for scaling | ipvs uses hash table that scales much better | scheduler can be configured: round robin, destination hashing, etc | there is actually a kube-ipvs0 dummy device that has the cluster IP | one IPVS virtual server per service IP | ipvsadm -ln gist | . | cli, reference kube-proxy command line reference | . | chiao, detail Cracking kubernetes node proxy (aka kube-proxy) | . | . | cAdvisor monitoring agent | kubectl proxy &amp;&amp; GET /api/v1/nodes/${1}/proxy/metrics/cadvisor | . | . Arch . High Level Architecture . . 2 . &lt;img alt=”another arch” src=”http://cythumb.cyworld.com/810x0/c2down.cyworld.co.kr/download?fid=64224fe7c9ab420678b250019c2ac900&amp;name=2015_09_25_kubernetes_architecture_with_flannel.png” width=75%&gt; . 3 . . with edge router . . Functionality . Ring Graph . . Health check . liveness probe checks if the container is broken and should be restarted this is only necessary if the application is not able to crash itself. | . | readiness probe check if the service is ready, e.g. depending service like mysql, redis, are ready | k8s will only send traffic to the pod if it’s ready | . | both probes are periodical | both support three types of diagnose: http, tcp and command line | deploymentspec.restartPolicy values can be Always (default), OnFailure, and Never | applies to all containers | . | article by ianlewis | think twice before using helm | . Composing . pod definition in YAML | also possible to use JSON | . Configuration Management . config map and secretes | secretes can be accessed from volume files or environment variable | updated secretes are NOT handled automatically | ConfigMaps are meant for non-secure informations can support JSON blobs and entire files | . | ConfigMaps can be used as envvar | file in volume automatically updated but it takes a while | . | docker run command line | . | giantswarm | k8s技术社区 | . Service . session affinity can be specified in service spec spec.sessionAffinity, value can be ClientIP or None | generates separate iptable run from kube-proxy | . | when a service is created without selectors, the Endpoints object is not going to be created and has to be defined by the user suitable for defining external services | . | service handles layer 4 routing, while ingress handles layer 7 | service can expose more than one port each port shoud be named in this case | . | service can be discovered by environment variable when a new pod is created, existing service will be available in the pod via envar | XXX_SERVICE_HOST, XXX_SERVICE_PORT | . | service can also be discovered by DNS | . Service types . ClusterIP this is the default type | enable pod to pod communication | provides service discovery | provides DNS | only accessible from inside the cluster | None: for headless service | &quot;&quot;: automatic assigned from a special IP segment | x.x.x.x: pre-allocated IP | . | NodePort a port on a node | externally exposed service | once exposed, the port is available on all nodes | builds on top of cluster ip and route request from all nodes on that port | can be specified by user, default is automatic default automatic range is 30000-32767, user configurable | . | . | LoadBalancer external IP acting as a load balancer | actual load balancer provided by the cloud provider | builds on top of NodePort and creates an load balancer | type: LoadBalancer | kubectl expose --type=LoadBalancer | . | ExternalName DNS alias for external services | this doesn’t involve kube-proxy | no port or endpoint is defined | my-service.prod.svc.CLUSTER points to name in spec.my.database.example.com | add an CNAME record into kube-dns | . | . Headless service . service with ClusterIP set to None | no IP will be configured and thus no iptable rules will be created | DNS entries are added depending on whether there is selector or not | . Deployment . spec.strategy controls upgrade strategy | .type: Recreate or RollingUpdate (default) | . | kubectl rollout status deploy/foo check rollout status | . | kubectl rollout history deploy/foo check rollout history | . | kubectl rollout undo deploy/foo rollback to previous version | can also rollback to a specific version with e.g. --to-version=2 | . | a rollout is triggered when part of .spec.template is modified | . DaemonSet . ensure that a pod is available on selected nodes | RestartPolicy has to be Always | spec.nodeSelector can be used to select node | . Jobs . kind: Job | pods that run for a while and stop and not to be restarted | 3 kinds non-parallel jobs | parallel jobs with a fixed completion count spec.completions controls how many completions is desired | . | parallel jobs with a work queue: spec.parallelism controls number of pod running at the same time | . | . | . CronJob . kind: CronJob | spec.schedule: &quot;*/1 * * * *&quot; | available from 1.5+ | API server has to be started with --runtime-config=batch/v2alpha1=true | . Init Containers . a feature added in 1.5 | within a pod, a set of containers can be specified to run before the app containers. | different to regular containers run to completion | runs one after another | . | metadata.annotation.pod.beta.kubernetes.io/init-containers 1.5 + | . | spec.initContainers 1.6 + | . | . Lifecycle Hooks . postStart in container spec called right after container creation | blocking all following operations | no gurantee that it’ll be triggered before ENTRYPOINT | no parameter | . | preStop in container spec blocking, stop only happens when hook handler returns | . | hook handler can be shell script: exec | HTTP: httpGet | . | hooks are delivered at least once | openshift blog about pod lifecycle | . | . Affinity . spec.nodeSelector provides simple way of scheduling pods on host | node affinity introduced in 1.2, rather flexible syntax allowed | inter-pod affinity introduced in 1.4 | can be nodeAffinity | podAffinity | podAntiAffinity | there is no nodeAntiAffinity | . | spec.affinity.nodeAffinity requiredDuringSchedulingIgnoredDuringExecution | preferredDuringSchedulingIgnoredDuringExecution for preferred, weight is required | . | . | operator supported In | NotIn | Exists | DoesNotExists | Gt | Lt | . | . Taint and toleration . related to node/pod affinity | taint roughly means mark | kubectl taint nodes node1 key=value:NoSchedule pod cannot be scheduled on this node node1 | unless there is a toleration that matches the KV pair and effect NoSchedule | . | effect NoSchedule no pod will be scheduled | PreferNoSchedule same as above but soft | NoExecute evict pod from node | . | toleration is declared inside a pod spec to make exception to the above rule key | value | operation: Exists, Equal | effect | . | . StatefulSet . mimics virtual machine stable, unique network ID: {statefulset-name}-{ordinal} | each pod gets: podname.headless_service_name as DNS | . | stable, persistent storage | stable means persistence across pod reschedule | . | used to be called PetSets | beta from 1.5 | includes a headless service that controls the domain $svc.$ns.svc.cluster.local | stateful set, the application itself, e.g. nginx | volume claim templates, each replicate will have its own PVC | . | meant for service with a pre-defined cluster size | network attached shared storage | . | replicas will be brought up one after another: {0 ... N-1} deletion reverses the order | . | nodes can communicate with one another through stable network names point of the headless service | . | can be scaled through hpa here | how successful this operation is depends on the actual application that’s being scaled | when scaling down, the pvc and pv won’t be removed, so that when it’s scaled up again, they’ll be reused, retaining the data | . | each pod has stable hostname statefullsetname-ordinal | has $podname.$service-name | . | leadership select can thus be performed | data on pvc will be retained even if the set is scaled down or deleted | spec.serviceName must point to the headless that pre-exists pod get DNS entry like pod-specific-string.serviceName.default.svc.cluster.local | . | terminationGracePeriodSeconds cannot be 0 | blog | mysql github repo for running mysql cluster | . | newstack deploying wordpress with statefulset and pv | . | cockroach example deploy | . | . ConfigMap . config can be exported to envar | command line argument | config file in volume | . | creation kubectl create configmap my-config | --from-literal=literal-key=literal-value | --from-file=ui.properties | --from-file=path/to/config/dir | . | consumption valueFrom: {configMapKeyRef: bla} | volumes[].configMap | . | . Namespace . by default, there are 2: default and kube-system | one can create new namespaces with YAML file (kind: Namespace) or directly with kubectl create ns | . | kubectl config set-context &lt;context&gt; --namespace=test sets the current namespace | . | kubectl create -f foo.yaml --namespace wisebuild create objetcs inside a namespace | . | namespace can also be specified in YAML files | namespace cannot be nested | network is not firewalled between namespaces | . DNS . &lt;service-name&gt;.&lt;namespace-name&gt;.svc.cluster.local | name must be DNS-1035 valid: &#39;[a-z]([-a-z0-9]*[a-z0-9])? this applies only to service and not pod or deployment | . | hostname.subdomain.namespace.svc..... pod.spec.hostname over pod.metadata.name | . | kubectl run --generator=run-pod/v1 tmp-shell --rm -i --tty --image nicolaka/netshoot -- /bin/bash handy command to run netshoot | . | . Ingress . an Ingress is a collection of rules that allow inbound connections to reach the cluster services. | internet ==&gt; Ingress ==&gt; Services | benefits: virtual domain | TLS termination | . | requires a ingress controller to be deployed on the master node | a ingress controller: is a reverse proxy that’s kubernetes aware | watches creation/update/deletion of rules | configure itself accordingly | not part of kube-controller-manager | officially nginx controller is supported as well as GCE | requires default backend for ingress | . | ingress resource kind: Ingress | spec.rules | . | multple ingress controller can run in parallel annotations: kubernetes.io/ingress.class: &quot;gce&quot; | . | . | nginx ingress controller receive events from k8s and update config file | reload configuration when needed | borrows stuff from openresty | customizable from annotation | . | . Philosophy . . Links . traffik official user guide for traffik | . | hackernoon pratical article of using trafik as a ingress controller | deploying trafik only on edge router nodes is a nice idea | . | crondev nginx controller | . | deploy official deploy document | on baremetal, it’s a deployment with nodeport | . | speakerdeck, ingress, dns, ipv6, thockin SIG-Network Deep-Dive, KubeCon EU 2019 | a large part of it is what’s wrong with current ingress and how to fix them. | . | . Downward API . mechanism to expose pod level information to container | info can be exposed to envar | volume file | . | envar env.valueFrom.fieldRef | fieldPath: spec.nodeName | fieldPath: metadata.namespace | . | . Autoscaling . HPA | kind: HorizontalPodAutoscaler | kubectl can specify policies directly | kubectl get hpa | seems to only support CPU at the moment | require heapster for collecting metrics in turn requires storage solution like influxdb | . | . Admission Webhook . hook for customized logic on job admission job filtering (reject/allow) | object injection | mutation - modify object | . | two types: MutatingAdmissionWebhook, ValidatingAdmissionWebhook | just a basic HTTP server that works with a particular API has to be TLS enabled so preparing certs is a pain | k8s API server uses CA bundle to access secure webhooks server | . | configured through: ValidatingWebhookConfiguration | MutatingWebhookConfiguration | . | different from initializer | usage: mutate resource before creating them | automatic provisioning of storage class | validation | namespace restriction | . | debug kc describe rs .... can see something | . | ibm Diving into Kubernetes MutatingAdmissionWebhook – IBM Cloud – Medium | nice diagram | actual api | . | github mutating tutorial code | . | github official test/sample code in 1.9 release | . | istio where istio injects its own sidecar | . | banzaicloud blog post about using admission hooks | . | kubewebhooklok Go framework that makes creating a hook easier | makes writing webhook very easy but … the hard part is preparing certs | . | API and CLI | v1beta.AdmissionReview | v1beta1.AdmissionResponse | admissionregistration.k8s.io/v1beta1 | kc delete mutatingWebhookConfiguration | . Flow (from Banzai Cloud) . &lt;img src=”http://borlandc.pek3b.qingstor.com/k8s/k8s-webhooks.png” width=75%&gt; . Initialiser . can’t be used for DELETE | ibm | ahmet | . Networking . three networks infra network that connects nodes | service network: pure virtual network, handled by firewall rules | pod network | . | each pod has its own IP address This is a balanced design between IP per host and IP per container | . | no NAT allowed between containers or containers and node (minion) | kubernetes has the concept of cloud provider, which handles load balancing | routes | . | in general, two ways of achieving connectivity overlay | direct | . | solutions include flannel | calico | romana | weave | canal = flannel + calico | open vswitch | . | comparison compares net host, ipvlan and flannel | a bit dated | . | google sheet large table with CNI plugins side by side | . | kubedex a lot of vendors uses Calico by default | . | . CNI . invoke: runtime creates network namespace | runtime checks the type field in JSON file and invokes the right plugin | . | a CNI plugin is responsible for: insert network interface into a network namespace (e.g. container) | arrange things on the host (e.g. attach veth to a bridge) | assign IP (invoking IPAM plugin) | | IPAM plugin is separate IPAM plugin code | dhcp, host-local and static | . | each plugin is implemented by a binary normally the plugin binaries are installed /opt/cni/bin | NOTE that these are plugin binaries, not daemon/controller binaries like flanneld | . | --network-plugin=cni | /etc/cni/net.d as conf directory | github GitHub - containernetworking/cni: Container Network Interface - networking for Linux containers | . | plugins github GitHub - containernetworking/plugins: Some standard networking plugins, maintained by the CNI team. | main contains default plugins like bridge, macvlan, ipvlan, etc | meta contains flannel, calico, etc | . | actual spec cni/SPEC.md at master · containernetworking/cni · GitHub | . | dasb Das Blinken Lichten · Understanding CNI (Container Networking Interface) | . | dasb Das Blinken Lichten · Using CNI with Docker | . | altoros Kubernetes Networking: How to Write Your Own CNI Plug-in with Bash | Altoros | . | shell script for managing bridge, subnet, allocating new IP, etc | . | plugin can have delegate the flannel plugin delegates bridge creation and IPAM to the bridge plugin | . | plugin can be chained looks like it started with CNI spec 0.3.0 | plugins: [{type: calico}, {type: portMap}] | also called config list | . | . Plugins . bridge find the bridge (default to cni0) | connects both ends of veth | . | macvlan a virtual device enslaved to a physical device, e.g. eth0 | WLAN device cannot be enslaved to | works with a IPAM plugin | supports a lot of different mode, defaults to bridge not sure what the different modes are | . | . | ipvlan | ptp point to point between container and host | . | host-device move an existing device into container netns | . | loopback creates the lo device in netns | . | flannel works with flanneld | delegates to bridge plugin | default bridge configuration is to use host-local IPAM | . | . IPAM Plugins . host-local discussion on static IP | . | static seems useless as it can only give on set of static IP | . | DHCP | . Plugin API . ADD INPUT: network namespace, network configuration | . | DELETE | VERSION | GET | . Flannel . etcd backed overlay | cross host shared routing table stored in etcd | each node has its own subnet | flanneld manages the subnet | distribute IP address for each pod | . | configuration exposed with /run/flannel/subnet.env | on host cni0 is like docker bridge that all pods connects to | flannel.1 is a virtual device that wraps all data in VXLAN (if necessary) and routes all data to eth0 on the host | . | laputa very detailed explaination how flannel works | container’s gateway is cni0, packets goes to cni0 first (10.96.1.1/24) | routing rule forward the packet to flannel.1 (10.96.1.0/16) | flannel.1 is a TUN device, which is a kernel virtual L2 device | all incoming/outgoing packets gets forwarded to flanneld | flanneld knows how to forward/tunnel the packets to which hosts, as all flanneld are connected to etcd cluster where such information is stored | etcdctl ls /coreos.com/network/subnets | . | alicloud flannel alicloud VPC backend | . | . Calico . Layer 3 model | Uses BGP | No NATting | . Weave . Overlay networking | Compatible with both libnetwork and k8s | . Canal . canal = calico + flannel | company behind it is called Tigera | technically it’s possible to combine networking from flannel and policy side from calico | news | . kubenet plugin . simple plugin implementation | no cross-node networking by itself | implements: cbr0 | . DNS . services each normal service has an DNS entry like foo. resolves to cluster IP of the service | . | headless service has an DNS entry like foo. resolves to IPs from all end point that’s associated with the service | client side round robin expected | . | FQDN is service.namespace.svc.cluster.local cluster.local part can be configured with kubelet | . | . | pod each pod has DNS entry like 1-2-3-4.default.pod.cluster.local where 1-2-3-4 is the IP of the POD | spec.subDomain can be used to specify subdomain, after which, hostname.subdomain.ns.cluster.local is in the DNS records | . | each pod can have a dnsPolicy defined Default: inherit configuration from node | ClusterFirst: queries are sent to kube-dns service this is the default behavior | . | ClusterFirstWithHostNet ??? | . | deployed as an add-on github md | . | /etc/resolve.conf adds a lot of search for k8s namespaces search default.svc.cluster.local svc.cluster.local cluster.local wise2c.com | . | actual records are stored in etcd for internal resolver to query through HTTP | dnsmasq handles caching of records | even if a pod is using host network, DNS resolving works as expected | it is possible to setup sub domain servers and upstream servers here | . | sysdig tracing DNS using sysdig | . | trouble-shooting kubectl get pods --namespace=kube-system -l k8s-app=kube-dns | kubectl logs --namespace=kube-system $(kubectl get pods --namespace=kube-system -l k8s-app=kube-dns -o name) -c kubedns | kubectl logs --namespace=kube-system $(kubectl get pods --namespace=kube-system -l k8s-app=kube-dns -o name) -c dnsmasq | kubectl logs --namespace=kube-system $(kubectl get pods --namespace=kube-system -l k8s-app=kube-dns -o name) -c healthz | . | spec kubernetes DNS spec | zone is like cluster.local | dns-version TXT stores schema version | A record for svc.ns.zone | for headless service, svc.ns.zone resolves to endpiont IP | . | internal what’s in the pod: etcd, kube2sky, skydns, exechealthz | where in etcd is the records | . | . Networking Policy . introduced in 1.3, GA in 1.7 | ingress policy only as of now | apiVersion: networking.k8s.io/v1 | kind: NetworkPolicy | spec.podSelector pod that’s affected | empty means all | matchLabels or matchExpressions[] | . | spec.ingress[] empty means nothing is allowed | list of white listed items | ingress[].from.podSelector | ingress[].from.nameSpaceSelector | ingress[].ports empty or missing means all ports, i.e. not restricted | otherwise white listing | . | . | . Storage . in k8s’ term, persistent disk | supports: docker volume | GCE disk | AWS EBS | NFS | gitrepo, clones a git repo | . | supports for cloud storage is specified as spec.volumes e.g spec.volumes.gcePersistentDisk | this implies that the core kubernetes code understands the syntax | so FlexVolume might be the only option when one wants to add customized volumes | . | PV reclaim policy Retain, where the volume is in a released state but the data is retained and can be recovered. | Delete, where the volume is deleted. | . | faq k8s volume plugin FAQ | three methods: in tree volume plugin | out-of-tree FlexVolume driver | out-of-tree CSI driver | | . | sheng yang rants about PV, PVC, StorageClass, etc by an Rancher engineer | special rants about Volume object | part-2 | verdict: use storage in the following order when possible: use Volume when necessary (cm, secrete, etc) | use provisioner where can | . | . | software engineering daily, simsek why is storage on kubernetes so hard | pretty decent summary of storage on kubernetes | CSI, rook, PV, PVC | . | . Volume types . emptyDir for scratch space | will be removed once pod is gone | use host storage by default, can be configured to use RAM | sizeLimit can be used to limit its size | . | hostPath similar to docker’s /hostpath:/containerpath | given target will be created as an empty directory owned by root if it does not already exist. | ? What if it already exists? | data only writable by root | ??? remove once pod is gone? | . | nfs | iscsi | secret | persistentVolumeClaim spec.storageClassName what type of storage to claim | . | gitRepo allow to specify git repo with commit hash | looks like this is not the right way and is deprecated | . | . PersistentVolume . aka PV | it’s like a node, which means that it’s a cluster resource provided by an admin | NOT namespaced!!! | procedure create persistent volume kubctl create -f | create persistent volume claim, status of that volume changes to BOUND | create pod that uses the volume | . | life-cycle static volume: provisioned by admin and available all the time | dynamic volume: when StorageClass is defined | volume.beta.kubernetes.io/storage-class: &quot;example-nfs&quot; | . | storage class PVC can request for a certain class of storage. E.g. google | PVC class has to match PV class for it to bound | in this case, the storage can be automatically allocated from the vendor | storageClassName | . | access mode ReadWriteOnce single node only, R+W | ReadWriteMany many nodes, R+W | ReadOnlyMany many nodes, R | . | . Chapter 6 . . Spec . spec.volumes emptyDir, hostPath, etc | . | spec.containers.volumeMounts mountPath: container path | name: which volume | . | . FlexVolume . github md | installation is by copying binary into a specific path on node /usr/libexec/kubernetes/kubelet-plugins/volume/exec/&lt;vendor~driver&gt;/&lt;driver&gt; | or it has to be /driver/driver | . | alpha from 1.2, GA from 1.8 supposedly replaced by CSI but k8s will continue supports it | . | API are defined as argument to the binary driver-exec &lt;args&gt;. Return of the API should be in JSON format init | attach in: json option, node name | useful for remote volumes such as EBS | this is optional, return Not supported in status string | . | mount: Mount device mounts the device to a global path on the host which individual pods can then bind mount. in: mount_dir, mount device, json options | . | unmount | detach in: json option, node name | optional | . | . | example are under git://examples/volumes/flexvolume | leebrigg nice introduction | . | rancher | cifs | . Dynamic Provisioning . out of tree dynamic pv blog | storage class is global, not namespaced | PVC is namespaced | . | objects involved StorageClass defines a new storage class backed by code provisioner has to match the actual POD envar for the handler to work | code is here | . | PersistenVolumeClaim with volume.beta.kubernetes.io/storage-class | triggers the Pod to dynamically create a PVC | . | . | . Internal . a controller is implemented that listens on volume creation and dynamically creates v1.PersistentVolume objects that will be handled by k8s | it creats a provision controller and run it non stop | . CSI . Container Storage Interface | an initiative to unify the storage interface of Container Orchestrator Systems (COs) like Kubernetes, Mesos, Docker swarm, cloud foundry, etc. combined with storage vendors like Ceph, Portworx, NetApp etc | cross orchestration platform | Bugs in volume plugins can crash critical Kubernetes components, instead of just the plugin. !!! | . | spec 1.0 markdown | . | blog, k8s Container Storage Interface (CSI) for Kubernetes GA | . | medium, google-cloud Understanding the Container Storage Interface (CSI) | nice colorful architecturual diagrams | . | pdf, yu jie | fatih, how-to How to write a Container Storage Interface (CSI) plugin | all functions have to be idempotent | . | driver, github https://github.com/kubernetes-csi/drivers | . | . Arch . &lt;img src=”http://borlandc.pek3b.qingstor.com/k8s/csi-arch-1.png” width=75%&gt; . API . gRPC | service Identity both node and controller plugin have to implement this | GetPluginInfo() | GetPluginCapabilities() | Probe() | . | service Controller controller plugin must implement this | cloud providers has to implement this | CreateVolume() | DeleteVolume() | ControllerPublishVolume() | ControllerUnpublishVolume() | ValidateVolumeCapabilities() | ListVolumes() | GetCapacities() | ControllerGetCapabilities() | . | service Node node plugin must implement this | NodeStageVolume() | NodeUnstageVolume() | NodePublishVolume() | NodeUnpublishVolume() | NodeGetId() | NodeGetCapabilities() | . | . API Authentication and Authorization . API requests are tied to a user (see below) or executed anonymously | 2 kinds of user: normal user and service account. service account is managed by kubernetes, normal user is not | Authz node | APAC | RBAC | Webhook | . | . Service Account . injects auth info into pods to talk to kubernetes services like the apiserver spec.serviceAccountName in pod spec | . | this is a namespaced resource | as oppose to user account that’s for humans | when no service account is specified for a Pod, it’ll be default | default service account is injected into /var/run/secrets/kubernetes.io/serviceaccount namespace | ca.crt (the global root certificate), allows secure communication with API server | token, says who you are and what you can do | . | . Nice Picture . . RBAC . role based access control | introduced in 1.6 as beta | apiserver needs to start with --authorization-mode=RBAC | 4 kinds of object defined Role | ClusterRole | RoleBinding | ClusterRoleBinding | . | a Role is a collection of permissions. by default a Role is bound to a namespace | there is ClusterRole that applies across the cluter | . | a RoleBinding binds a role to a subject subject can be user | group | service account | . | binding can be namespaced or cluster level across namespaces | . | CLI kubectl get clusterroles --namespace=kube-system | . | official blog | . OIDC Plugin . CLI --oidc-issuer-url=URL | --oidc-client-id=ID | --oidc-username-claim=email | --oidc-groups-claim=groups | . | dex, k8s Kubernetes authentication through dex | . | . Links . k8s auth | rbac, banzaicloud mainly talks about LDAP RBAC integration but has nice explannation of authenticating with API server | . | . Resource Definition File . YAML file that presents a spec for a resource can also be JSON | . | Sections apiVersion | kind | metadata | spec | . | NOTE: it’s not possible to use environment variable in spec file | . kubectl . kubectl supports one version forward and backward skew | looks for $HOME/.kube/config file for configuration can also be specified with --kubeconfig flag | . | export KUBERNETES_MASTER=http://host01:8080 defines master to communicate with | . | kubectl get pod pod1 | kubectl get pods | kubectl get pod -l run=my-nginx get pod with specific label | . | kubectl get deployments | kubectl config use-context xxx swtich context (environment) | . | kubectl run my-web --image=nginx --port=80 creates a deployment object | . | kubectl expose deployment my-web --target-port=80 --type=NodePort creates a service that exposes a deployment | allows external access through NodePort | . | kubectl scale deployment hello-node --replicas=4 | kubectl get svc list services | . | kubectrl create -f storage-memory.yaml create pod, namespace etc | . | kubectl create namespace staging create new namespace | . | kubectl exec foo-37kj5 -i -t -- sh like docker exec | . | kubectl exec -it two-containers -c nginx-container -- /bin/bash specifies pod and container | . | kubectl logs foo-37kj5 check log from a pod | -c to specify a container | . | kubectl get pod nginx -o go-template=&#39;&#39; get IP of pod named nginx | . | kubectl describe namespace/test describe stuff | other stuff maybe rc/busybox-ns | kubectl describe service ghost | . | kubectl set image deployment/hello-node hello-node=gcr.io/$PROJECT_ID/hello-node:v2 upgrade a service by setting a new version of image | won’t work if the image name stays the same, e.g. :latest | . | kubectl delete pod nginx kubectl delete pods &lt;pod&gt; --grace-period=0 --force to force it | . | kubectl delete rc nginx delete replication controller | . | kubectl delete svc nginx delete service | . | kubectl logs -c container pod | kubectl config use-context k3 switch cluster | . | kubectl cordon $NODENAME make node unschedlable | . | kubectl get nodes -o jsonpath=&#39;{range.items[*].metadata}{.name} {end}&#39; | kubectl get pod liang-web-3685169472-vbxh6 -o jsonpath=&#39;{.status.podIP}&#39; specific information retrieval | . | kubectl patch node k8s-node-1 -p &#39;{&quot;spec&quot;:{&quot;unschedulable&quot;:true}} patch a node so that it’ll not be scheduled on | kubectl patch (-f FILENAME TYPENAME) -p PATCH | . | kubectl edit deploy/orchestration fire up editor and edit YAML definition of a deployment | . | kubectl convert -f pod.yaml --output-version v1 convert/migrate file between API version | can be done locally or through API server | schema are stored under $HOME/.kube/schema/ | . | kubectl explain --recursive pod describes what a pod definition is | . | kubectl get pods --all-namespaces -o custom-columns=NAME:.metadata.name,NAMESPACE:.metadata.namespace,QOS-CLASS:.status.qosClass customized field | . | kubectl get rs,secrets -o json --namespace old | jq &#39;.items[].metadata.namespace = &quot;new&quot;&#39; | kubectl create -f - | kubectl get node -v8 very verbose, good for debugging | . | kubectl -n istio-system port-forward $(kubectl -n istio-system get pod -l app=grafana -o jsonpath=&#39;{.items[0].metadata.name}&#39;) 3000:3000 port-forward forward one or more local ports to a pod | . | kubectl cordon make the node unschedulable | same as node.Spec.Unschedulable=true | . | kubectl drain first it cordon the node | then does evict of pods 1.7 . | . | pods can be filtered, e.g. --ignore-daemonsets | banzaicloud | . | kubectl get secret gitlab-registry --namespace=revsys-com --export -o yaml | kubectl apply --namespace=devspectrum-dev -f - copy objects betwen namespaces | . | . Troubleshooting . kubectl get cs get componentstatuses for master: scheduler, controller manager, etcd | . | kubectl top requires heapster to work | . | . Security . kubectl certificate approve | kubectl certificate deny | . Configuration . $HOME/.kube/config includes configuration for all contexts | it has several sections cluster | context | user | . | to get a new cluster into the config file, one has to merge content of admin.conf into it | kubectl --kubeconfig ./admin.conf get nodes config file is generated by kubeadm in /etc/kubernetes/admin/.conf | . | use the following combination kube-ps1 + kubectx + kubens!!! | . | medium, master merge, extract, direct use of username, password | . | . CLI . # Save certificates and the key as files $ kubectl config set-cluster default-cluster --server=https://45.32.47.214 --certificate-authority=${CA_CERT} $ kubectl config set-credentials default-admin --certificate-authority=${CA_CERT} --client-key=${ADMIN_KEY} --client-certificate=${ADMIN_CERT} $ kubectl config set-context default-system --cluster=default-cluster --user=default-admin $ kubectl config use-context default-system . Plugins . loaded from predefined directories: $HOME/.kube/plugin | e.g. .kube/plugin/hello/plugin.yaml has to follow this naming | . | kubectl plugin hello runs the plugin | . Links . kubectl man pages | installation with manual download instruction | . | . YAML . Meta . apiVersion | kind pod | rc | Deployment | Service | etc | . | label | . Spec . containers: type list image | name | ports has name!!! | . | . | . Sample . apiVersion: apps/v1beta1 kind: Deployment metadata: name: web-deployment spec: replicas: 2 template: metadata: labels: name: web spec: containers: - image: gcr.io/&lt;YOUR-PROJECT-ID&gt;/myapp name: web ports: - name: http-server containerPort: 3000 . API . Overview . resource catagory workloads | discovery &amp; LB | config and storage | cluster | metadata | . | resource objects spec | status | meta | . | resource operation create | read | delete | rollback | . | workloads deployments: for stateless apps | statefullset: for persistent apps | jobs: run-to-completion apps | . | . Convention . Kind | Resource | API Group group/version | . | . Links . 1.6 pod spec | 1.6 container | 1.6 deployment | 1.6 service | openshift 3.7 REST API reference | not directly k8s but should be the same | . | . Proxy . kubectl proxy -p 8001 generates a proxy server that can be used as REST API server | handles authentication | curl http://localhost:8001/api/v1/namespaces/default/pods | . | . UI . kube-web-view live demo | . | k8dash | kubernator | dashboard | kube-ops-view ops oriented, with node, cpu, ram as center view | python, js | . | kube-resource-report node cost for cloud resources | Python, html (static site) | . | octant, vmware … | . | comp, blog | . Install . get | binary | each node runs: docker | kubelet | kube-proxy (not strictly required on a master node) | . | kubernetes services, each run as a pod on the master node apiserver | controller manager | scheduler | all runs the same hyperkube binary with different swtich hyperkube is like busybox | . | . | . kubernetes the hard way . download cfssl and cfssljson | prepare compute instances (gcloud specific) | create CA cfssl gencert -initca ca-csr.json | cfssljson -bare ca | creates ca.pem and ca-key.pem | . | create admin client certificate cfssl gencert -ca=xxx -ca-key=xxx -config=xxx -profile=xxx admin-csr.json | cfssljson -bare admin | ca and ca-key is generated in previous step | creates admin.pem and admin-key.pem | . | likely, create api server certificate | likely, create kubelet client certificate | distribute certification copy ca.pem and instance certificates to each minion | copy ca.pem, ca-key.pem and api server certs to each controller | . | configure encryption configuration | bring up etcd sudo cp ca.pem kubernetes-key.pem kubernetes.pem /etc/etcd/ | configure systemd etcd service | . | bring up kubernetes control plane sudo mv ca.pem ca-key.pem kubernetes-key.pem kubernetes.pem encryption-config.yaml /var/lib/kubernetes/ CA key pair | api server key pair | encryption config | . | configure apiserver with systemd | configure controller manager | kubectl get componentstatuses | . | bring up node download cni binary | download crio binary | configure network | configure kubelet | configure kubeproxy | . | deploy DNS add-on | link | . minikube . brew install Caskroom/cask/minikube | minikube start | minikube dashboard | . kubeadm . automated installation tool, handles preflight check | PKI creation | generates token used for node to join | manages kubelet running options | . | by default, images are pulled from gcr.io/google_containers apiserver | controller-manager | scheduler | proxy | etcd | pause | dns-sidecar | kube-dns | dns-masq | . | installs DNS add-on can pick from default and coredns | . | when KUBE_HYPERKUBE_IMAGE is defined, a single hyperkube image is used | configuration can be done through file | reference | ntp ntp should be installed on all nodes | . | . Install . normally with apt or yum | . Configurable . --pod-network-cidr | etcd server either through image envar | or config file | . | repo prefix KUBE_REPO_PREFIX | when specified, --pod-infra-container-image has to be change too | . | . Each host . docker | kubelet | kubectl (master only) | kubeadm | cni | . Steps . install docker | install kubernetes packages (apt) kubelet | kubectl | kubeadm | . | disable SELinux | on master, do kubeadmin init --api-advertise-addresses=&lt;host_ip&gt; --use-kubernetes-version=v1.5.3 --pod-network-cidr=10.244.0.0/16 | kubectl apply -f kube-flannel.yml | normally kubernets’ images are self contained. but if there is mismatch between versions, do export XXXX_IAMGE=bla | . | on each node flannel images has to be present on each node | kubeadm join --token=d562d2.bf3721e0655d4f12 192.168.0.177 | . | kubctl get nodes on master should show all nodes | Internal . with control plane, it uses kubelet to managed components like etcd, apiserver etc systemd runs kubelet as a normal process | kubelet --pod-manifest-path /etc/kubernetes/manifests starts everything in that directory as pods files in that folder are simply kubernetes YAML spec that kubelet understands | see /etc/systemd/system/kubelet.service.d/10-kubeadm.conf | . | . | how does it work | . OKDC . dockone, okdc | no need to install docker | install lsb | make sure /etc/hosts has all the nodes | setenforce 0 | on master just run the script | . | on node at the end of the master run it actually gives command to run on the node | . | . commands . apt-get update curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - cat &lt;&lt;EOF &gt; /etc/apt/sources.list.d/kubernetes.list deb http://apt.kubernetes.io/ kubernetes-xenial main EOF apt-get update apt-get install -y docker.io apt-get install -y kubelet kubeadm kubectl kubernetes-cni service kubelet stop service kubelet start kubeadm init --use-kubernetes-version v1.4.1 . Security . having SSH account into container is bad practice | kubeaudit automatically audit security in a kubernetes cluster | . | . Scheduling . PDB . poddisruptionbudget | specifies tolerated number of pods to keep alive, making sure that there are some left kubectl create pdb my-pdb --selector=app=nginx --min-available=70% | . | this will be observed by pod eviction which is the difference between delete and evict | . | . Resource . v1/ResourceQuota object one or more per namespace | . | once quota is enabled, user must specify request or limits for those values | --admission-control= | quota can be set for computatation resources as well as kubernetes objects like service, etc | requests.foo makes explict request for resources | limits.foo specifies explict limits for resources | foo can be cpu, and memory, etc | memory is total resident set size (RSS) and page cache, swap is disabled so not included here pretty nice article, explaining memory limit | . | so, RSS | . | 1 CPU == 1 hyperthread in a Intel processor | LimitRange sets resource usage limits for each kind of resource in a Namespace. demo | . | sysdig, oom, trouble How to troubleshoot Kubernetes OOM and CPU Throttle | Despite this mechanism, we can still finish up with system OOM kills as Kubernetes memory management runs only every several seconds. If the system memory fills too quickly, the system can kill Kubernetes control processes, making the node unstable. | using CPU can never be the reason of Kubernetes killing a container. | ❓ not sure I fully understand the CPU throttling part | . | . Security Context . defines privilege for a Pod as part of a pod spec | runAsUser: 1000 | capabilities | . Pod Security Policy . Secrets . v1/Secret | defined as k/v pairs | kubectl create secret &lt;docker-registry, generic, tls&gt; | . Access . volume: volumes.secret mounted as directory matching secrete name, under which | file name matches key | file content matches value | . | envar: env.valueFrom.valueFrom with name and key to access the value | . | . CLI . kubectl get quota --namespace=myspace | kubectl describe quota compute-resources --namespace=myspace | kubectl describe quota object-counts --namespace=myspace | . Links . official | official practical | . Monitoring . 4 level of monitoring host | containers | kubernetes | applications | . | cAdvisor, kubelet -&gt; heapster -&gt; backend backend supported: influxdb | opentsdb | kafka | many others | . | heapster run as a pod in the cluster | . | . cAdvisor . runs as part of kubelet | . Links . datadog However, as explained in the first section of this post, to track memory and CPU usage you should favor the metrics reported by your container technology, such as Docker, rather than the Kubernetes statistics reported by Heapster. | . | . Container Runtime . containerd blog containerd integration goes GA | . | . Arch . . CRI . container runtime interface for kubernetes | parallel to CNI, CSI, etc | implementations containerd (cri, cri-containerd) | CRI-O: OCI conformant, used to be called OCID, RedHat, etc | frakti (hyper.sh) | . | openstack, PDF, PPT topic is Frakti, but has a good bit of information on CRI, with history, landscape, etc | kata | . | suse, cri-o, kubic CRI-O is now our default container runtime interface | . | infoq CRI-O: An Open Source Container Runtime for Kubernetes | . | . Spec . Sandbox Create | Delete | List | . | Container Create | Start | Exec | . | Image Pull | List | . | . CRI-O . .io | conmon handles monitoring, including logging | podman is like docker CLI | docker and podman can co-exist on the same host | CRI-O 1.13 shipping with OpenShift 4.1 as the only supported engine | . containerd . container runtime | as a daemon | responsible for image transfer | container execution | network attachment | . | full OCI support | depends on runc | stuff are namespaced within kubernetes, k8s.io is the default namespace | . | spun out of docker into CNCF | . | github | cri plugin crictl | sig crictl | . | . rurnc . runc is a CLI tool for spawning and running containers according to the OCI specification. | . Rancher . replace SkyDNS with Rancher DNS allow cluster to expand over different resource pools and clouds | . | . Helm . for managing k8s charts | uses Go template syntax | a repo is a place where charts are stored | a chart is like a .deb file includes all related kubernetes resources | . | release a chart instance | same chart can be installed more than once | . | hooks e.g. post-install, post-upgrade | operation can be any k8s object | annotations: { &quot;helm.sh/hook&quot;: ...} | . | very much like Rancher catalogue | benefits: parameterisation | application lifecycle hook | history of releases | . | charts github repo for charts | . | installation download binary from web | brew install kubernetes-helm | . | sweetcode, firstlook, helm3, lua a first look at helm3 plan | Lua to replace Go template for better readability | . | tools, 15 15 useful helm chart tools | helm diff, helmsman | . | codefresh, template, note helm is not only a template solution | . | . Arch . &lt;img src=”http://borlandc.pek3b.qingstor.com/container/helm-application-deployment-management-for-kubernetes-13-638.jpg” width=75%&gt; . CLI . helm init: creats and populate $HOME/.helm | . | helm repo update | helm repo add &lt;name&gt; &lt;url&gt; helm repo add bitnami https://charts.bitnami.com/bitnami | . | helm search | helm install helm install --name kubeapps --namespace kubeapps bitnami/kubeapps | normally the chart will prints out some information on how to use it afterwards helm status has same effect | . | . | helm rollback | helm list list all releases | . | helm delete &lt;release&gt; after deletion, helm status &lt;release&gt; still works, ns is still there | . | helm create creates skeleton for a new chart | . | helm reset uninstall helm | . | helm serve run a local helm repo server | . | . Chart . organized as a collection of files inside a directory Chart.yaml - metadata name, version, appVersion, keywords, etc | . | values.yaml - variable referenced in templates | charts/ - dependency chart | templates/ - your own template files | templates/_helpers.yaml stores utility macros | . | chart can have dependencies | default templating is done with Go template values.yaml includes default value | .Values - stuff in values.yaml | looks like variable name that starts with . is built-in | . | a chart can have subcharts | private char repo: helm package . from where Chart.yaml is | helm repo index . | devd . | helm repo add local http://127.0.0.1:8000 | it generates index.yaml and hello-1.0.0.tar.gz tarball has all the chart files | . | . | functions Files.Get() | . | jfrog jfrog 5.8 supports private helm chart | . | monocular search and discovery front-end for helm chart repo | has REST API and UI | can be used in wisecloud | . | kubeapps UI for helm | . | hackernoon using private github repo | . | . Helm 3 . removal of tiller | release name are namespaced | push chart into OCI registry | . High Availability . keith external etcd | master each has a keep alived | use kubeadm for installation | pull images from a pre-defined HTTP server | single BASH script for master and node | . | official mentioned monit for monitoring k8s and docker daemon | talked about etcd | api is replicated and load-balanced | scheduler and controller-manager which actually modifies the cluster state uses the --leader-elect flag to make sure only one is doing the work at a time | see ha with ansible in the link section | . | kargo | kubeform | sumit/podmaster works together with etcd | make sure that only the leader runs scheduler | generic solution but mostly works with kubernetes | . | . kompose . convert docker-compose file to Kubernetes files | by default, generateds deployments, service, etc in YAML can also generates replication controller, daemon set, or helm charts -rc | . | also possible to export in JSON format -j | . | helm is possible to -c | . | . | written in Go | kompose specific labels inside docker-compose file | kompose.service.type: nodeport, clusterip, loadbalancer | kompose.service.expose: true/hostname | . | . Unsupported syntax . build command dockerfile | . | cap_add | . Links . k8s blog introduction article | . | io official website | includes an architecture explanation that looks like the translator | . | conversion list of compose definitions and how it gets converted | . | . Internal . etcd is used for data storage and implementation of watch mechanism | only the API server access the etcd directly | . kubelet . it finds pods to run from the API server | it runs an internal HTTP server on port 10255 what is | provides health check /healtz | /pods for pods running on the node | /spec for the spec of the node | . | ./kubelet --api-servers=http://master:8080 | kubelet --cluster-dns specify cluster DNS server | it gets inserted into /etc/resolv.conf when DNS policy is clusterFirst | . | . scheduler . runs on master | watches API server for unbound pod and assign it to a node | ./kube-scheduler --master=http://localhost:8080 | it is possible to run customized scheduler howto | ibm | . | . controller manager . watches the shared state of the cluster through the apiserver and makes changes attempting to move the current state towards the desired state | . Pod creation sequence . &lt;img src=”https://cdn-images-1.medium.com/max/1600/1*WDJmiyarVfcsDp6X1-lLFQ.png” width=75%&gt; . Links . what even is a kubelet | . Troubleshooting . journalctl -r -u kubelet | kubectl get events -w -w for watch | . | . Extending . itnext patterns api extension mechanism | . | kubectl get apiservice list current API names | . | . CRD/TPR . task | primary API extension mechanism | looks like it doesn’t support versioning yet | . API Aggregation . incubator doc | full power of customization | require etc storage | . Custom Sub resource . a sub resource can be on a native or custom Kind An example of a custom sub-resource can be http_requests_total defined on a Pod will allow you to find out the number of HTTP requests received by that Pod. | . | . Admission Webhook . admission webhook | . Operator . k8s native way to run stateful application | basic work flow is: observe if the cluster has the right size | reconcile | rebalance data | . | in a nutshell reponsed to events created | updated | deleted | . | try to reconcile | . | operator SDK has CLI: operator-sdk | $ operator-sdk new &lt;operator-project-name&gt; --api-version=&lt;your-api-group&gt;/&lt;version&gt; --kind=&lt;custom-resource-kind&gt; | operator-sdk generate k8s | operator-sdk build &lt;docker-image&gt; | . | banzaicloud a complete guide to kubernetes operator SDK | how to use the operator SDK Create a new operator project | Define the Kubernetes resources to watch | Define the operator logic in a designated handler | Update and generate code for custom resources | Build and generate the operator deployment manifests | Deploy the operator | Create custom resources | | . | . Arch . . Links . python, operator kopf, from zalando | A Python framework to write Kubernetes operators in just few lines of code. https://kopf.readthedocs.io | . | youtube, hard Writing a Kubernetes Operator: the Hard Parts - Sebastien Guilloux, Elastic | schedule, pdf | operator lives into past (api server client uses a cached reader) | optimistic concurrency: resource name | resource version | uid: on deletion | . | . | . Windows . see dotnet.md | . VM, Virtualised Hardware . Frakti . based on Hyper | requires | youtube google drive companion slides | . | . | . kubevirt . implemented as a entirely new kind CRD | still needs PVC | blog getting to know kubevirt | . | . virtlet . from Mirantis | implemented as CRI daemonset is run on host where VM can be scheduled onto and handled separately from normal pods | . | runs arbitary QCOW2 images image names are translated from virtlet.cloud/ to a real image address | so probably it still needs PVs | . | mirantis, introduction | kubevirt vs virtlet kubevirt vs virtlet | . | dockerhub docker hub page has some nice info | . | persistent persistent root filesystem | example | . | . Arch . &lt;img src=”http://borlandc.pek3b.qingstor.com/k8s/virtletarchitecture.png” width=75%&gt; . kata container . … . Distributions . typhoon minimal and free | . | . Release History . 1.8 . blog | core workloads promoted to beta2 | RBAC out of beta | affinity moves out of annotation | . 1.9 . workloads AP GA deployment, daemonset, rs, and statefulset | apps/v1 is the new version | apps/v1beta2 deprecated | . | windows container support (beta) | ipv6 support (alpha) | coredns (alpha) | CRD (beta) | CSI (alpha) container storage interface | . | deprecation etcd2 support | default selectors. E.g. deployment.spec.selectors | volume.beta.kubernetes.io/storage-class annotation | . | supports IPv6 only cluster | raw block support | changelog | workloads | admission webhook beta | . | . 1.10 . changelog | coreos better, more organized changelog | . | local persistent storage diff from hostPath: configured through PVC, so no need to specify path on host in the spec | PV added PersistentVolume.Spec.NodeAffinity field | storage class added StorageClass.volumeBindingMode: WaitForFirstConsumer mode | spec | design | provisioner does NOT support dynamic provisioning | . | blog show manual way of declaring storage class, PV and PVC | . | banzaicloud using local PVC with kafka | configure where (mount point) local PV will be created | deploy daemonset for dynamic provisioning | create storage class | . | . | pod configurable resolv conf | pod process namespace sharing | API aggregation is stable | CSI becomes beta | TokenRequestAPI - identify pods | jetstack hidden gems | device plugin, like nVidia [Setting up a GPU Enabled Kubernetes for Deep Learning](https://itnext.io/setting-up-a-gpu-enabled-kubernetes-for-deep-learning-aef8e198931b enable GPU feature gate | on node: CUDA driver, install nvidia-docker2 | k8s: run nvidia/k8s-device-plugin:1.9 daemonset. github | resource limit: nvidia.com/gpu: 3 | . | . | core DNS beta | pids per pod (alpha) | shared PID namespace | CRD sub resource | . | . 1.11 . changelog | blog | coreos | coredns as default DNS provider | RH OpenShift 3.11 ships with this | IPVS GA cluster ip deep dive | . | cri-tools GA | pod priority and preemption control scheduling policy for pod when it shares node with k8s control plane | . | heapster deprecated can be replaced by metrics server | . | etcd2 deprecated | external cloud provider BETA | . 1.12 . a lot of security related updates certificate rotation | CIDR for network policy | mount space propogation | . | server side kubectl printing | egress support for network policy | beta: quota priority by namespace | custom metrics in HPA | update plugin mechanism for kubectl | pod vertical scaling | . | alpha runtimeClass: use different runtime | APIServer dry-run | volume snapshot | . | mirantis | rancher | itnext.io, hpa, metrics, prometheus HPA with custom metrics | . | . 1.13 . CSI PV support GA link | . | kubectl diff | kubectl plugin moves into beta | CRD supports multi version schema | dry run mode and enabled by default | heapster retired | uses IPv6 friendly coredns | official blog | . 1.14 . Windows server container support microsoft | graduated from beta to stable | . | kubectl kustomize integrated | plugin mechanism updated | server side apply | . | runtime class beta | pod priority and preemption (GA, was beta in 1.11) preemptive NonPreemptingPriority will be available in 1.15 | doc | create kind: PriorityClass, add priorityClassName to pod spec | grafana incident | . | configurable resolv.conf in pod github issue | . | ALPHA: provide environment variables expansion in sub path mount | ALPHA: In-tree storage plugin to CSI driver migration | something about local pv | sysdig | kontena part of their 2.4 release note | Support for HugePages | RuntimeClass (in beta), ability to select specific container runtimes for certain workloads | RunAsGroup to specify primary groups for the processes running inside the pod containers | Pod priority and preemption are graduating to stable | Pod ready++ to enhance pod readiness checks with possibly external view | Pid limiting | Configurable resolv.conf | Durable local storage management | . | . 1.15 . CRD pruning: removing fileds that are not in the validation schema | CRD defaulting: providing default value for missing fields | kubeadm now seamlessly rotating all your certificates (on upgrades) before they expire | go mod | preparing for removal of cloud-provider | Nodes now support third party monitoring plugins. | Pod Disruption Budget (PDB) into beta | node local DNS cache (Beta) | redisgn of event API (Alpha) | Add non-preempting option to PriorityClasses (Alpha) | Execution hooks | CSI: volume cloning (Alpha) This new feature allows users to use another Persistent Volume Claim (PVC) as a “DataSource” when provisioning a new volume. | . | new interactive release note format | google sheet for enhancement tracking | vmware | sysdig quite a long list of interesting details | . | [—] DaemonSet, Deployment, and ReplicaSet resources will no longer be served from extensions/v1beta1, apps/v1beta1, or apps/v1beta2 in v1.16. Migrate to the apps/v1 API, available since v1.9. Existing persisted data can be retrieved via the apps/v1 API. | . | . CVEs . 1.15.5: CVE-2019-11253 | . 1.16 . GA: CRD, more strict structural schemas | pruning unknown fields | validation | and protecting the *.k8s.io | CRD: apiextensions.k8s.io/v1 | . | GA: admission hook | CSI, beta: volume resizing | CSI: feature parity with in-tree driver | API, beta: server side apply moving kubectl apply to server side | . | NET, Alpha: ipv4, ipv6 dual stack | STORE, Beta: volume cloning | metrics overhaul seems might cause problems | keg | . | +, Alpha: Endpoint slice alternative to Endpoint resources | seems to apply to a service with large amount of pods | . | +, Alpha: Ephemeral containers you can add regular containers to a pod after creation | no ports allowed | no health check | no resource | official doc | . | API depractions (only beta version) extensions/v1beta1, extensions/v1beta2, apps/v1beta2 -&gt; apps/v1 | NetworkPolicy | PodSecurityPolicy | DaemonSet, Deployment, StatefulSet, and ReplicaSet | SelfLink removed from all objects | more detail | . | k8s.io | release note | sysdig | . 1.17 . topology aware routing of service | kubeadm machine/structured output | IPv4/IPv6 support | taint node by condition | ALPHA: topology aware service routing --feature-gates=&quot;ServiceTopology=true&quot; | . | BETA: EndpointSlice API | DEPRECATED: default service cidr, added --service-cluster-ip-range for api server | RBAC: alpha, beta moved to rbac.authorization.k8s.io/v1 | . | sysdig | k8s.io | changelog on github | relnote.k8s.io | enhancement tracking | . 1.18 . enhancement tracking | sysdig OIDC discovery | kubectl debug | kubectl diff | runtimeclass on Windows with multiple Windows build support | . | ContainerD on Windows | runAsUser on Windows | kubeadm join on Windows | endpoint slice | IPv6 to beta | Ingress to beta | . | magalix HPA velocity | Even Pod Scheduling (based on AZ) | kubectl debug | . | . CVEs . 1.16.2: CVE-2019-11253 | . Questions . With a hostPath volume, will it be removed when the POD dies? no. looks like the stuff is retained | . | . Links . intro | minikube run k8s cluster on a single host | . | why kubernetes has won | kubernetes api explaine pods as docker commands, ipc, net, etc | . | V1 API official API doc for V1 | . | ringstack moving node js to k8s | . | walkthrough official tutorial | . | awesome git book | . | cheatsheet | official cheatsheet from kubernetes.io | . | marantis introduction | . | 5pi lengthy article on setting up production ready cluster | . | from scratch | cheap way | kube news | kompose convert, deploy docker compose stacks into k8s | . | rootsong 这可能是目前为止最详细的kubernetes安装文档了。 | markdown | . | ubuntu Ubuntu Kubernetes distribution | supports local and remote cloud | . | HA with ansible HA k8s with Ansible | . | python API | marantis RC, RS and Deployments | . | demystify demystify kubernetes from slideshare | pretty detailed, ground-up explanation of kubernetes | . | qihu360 what can goes into deployment object | . | tryk8s free k8s cluster to play with | login with github account | . | service lb provide loadbalancer service for bare metal | . | jazz improve depth first article from CEO of heptio | . | coreos deploy master | rbac securing kubernetes cluter with RBAC | compares to ABAC | . | app-controller manages dependency | seems to create new kubernetes objects somehow appcontroller.k8s/v1alpha1 | . | feisky Chinese book on 1.6 | . | security best practice | mantl.io integrated platform including a lot of stuff such as kubernetes, mesos | . | cloudnativelabs enforcing kubernetes network with iptables | pseudo code for network policy operation, like adding to ipset | . | kube-gen generate file based on k8s event and metadata | . | kubewatch watch k8s resource and send notification to Slack channel | . | posta training docker and kubernetes 4 day training by Christian Posta | . | draft a tool that scans source code and generate k8s objects | help geneartes Dockerfile, k8s YAML files, etc | . | brigade, microsoft, javascript, event k8s event driven scripting using Javascript | builds container pipelines | k8s native | requires helm | . | life of a packet k8s networking video | speakerdeck by the same guy and Tim Hockin | not actually for kubernetes but still useful | . | . | which cni comments on various CNI | . | jolestar training video traingin given by a guy from Qingcloud | . | JDOS JD custmization of kuberntes | a lot of high level information but still readable | . | borg to kubernetes Youtube video | The key point of kubernetes and Borg is: resource scheduling optimisation | . | distro spreadsheet of k8s distributions | wise2c included | . | configuration management tools like helm | . | heptio kubernetes subscription integrated PaaS from heptio | . | gravitational kubernetes release cycle | LTS? | . | container runtimes Whose Job is it anywa, CRI-O | . | codeship roundup of managed kubernetes platforms | PDF | GKE, openshift, Ubuntu, etc | . | 50000 scale kubernetes to 50000 services | . | rancher PDF | talk by Jiang Peng | kubernetes upgrade | etc backup | health check | . | techbeacon one year using kubernetes | run data service like MySQL and Mongo outside of kubernetes | cost: extra requirement of etcd, master nodes | logs sent directly from container themselves | published in 2016 | . | ryan an interrogation of metaparticle | metaparticle: a leaky abstraction, breaks the law of sepration of concerns | . | newstack codeship kubernetes solution directory | . | coreos operator operator introduction | . | venturebeat kubecon EU 2018 summary | . | objectif ipvs and kubernetes | with a lot of pods on a host, ipvs performance much better than iptables: O(1) | also see this pdf | . | containerd kubernetes containerd integration goes GA | kubernetes 1.10 + containerd 1.1 | performance gain | docker engine will use containerd in the future | . | openai scaling kubernetes to 2500 nodes | etcd: move from networked storage to local SSD | etcd: increase hard storage limit | scheduler: schedule pods to a single node as much as it can | kubelet: --serialize-image-pulls=false to allow parallel image pull | docker: switch to overlay2 | docker: move docker root to SSD | docker: max-concurrent-downloads option to 10 | networking: hostNetwork: true and dnsPolicy: ClusterFirstWithHostNet. | networking: increased ARP cache size | . | eks AWS EKS GA announcement | . | infoq 宁辉 | mentions SR-IOV | kubernetes + IaaS | . | gke-on-perm private GKE | announced July 2018 | . | kubeiql GraphQL interface for kubernetes | . | kubergui GitHub - BrandonPotter/kubergui: Kubernetes GUI YAML generators for simple but typo-prone tasks | HTML yaml generator | . | freshtracks a deep dive into kubernetes metrics (6 parts) | . | kubefwd GitHub - txn2/kubefwd: Bulk port forwarding Kubernetes services for local development. | developer tool to forward all remote service from a namespace transparently | access /etc/hosts to make sure local dev can access remote service with same name | . | itnext CNI benchmark over 10G | calico, flannel, cilium, weave, canal | calico is good, cilium is confusingly bad | jumbo frame activated (MTU 9000) - MTU matters | . | rust, api somebody is starting to write a k8s API with Rust | . | tinder, production, scale report from Tinder on moving to kubernetes | cluster size, flannel optimisation, dns problem, using envoy to fix HTTP Keepalive | 200 services, 1,000 nodes, 15,000 pods, and 48,000 running containers. | . | google docs, cka lengthy google spreadsheet with training material | . | alibaba, open source, kruise Advanced StatefulSet, BroadcastJob, and SidecarSet open sourced by Alibaba | . | release, support Kubernetes Release Versioning | Furthermore, we expect to “support” three minor releases at a time. | . | discuss discuss | . | tim hockin, kube-proxy, diagram kube-proxy NAT iptable flow | . | github, multipass, install Multi-Node Kubernetes 1.17 with kubeadm on local multipass cloud with Docker, Containerd or CRI-O and Rancher Server on top | . | golden, tools, google The Golden Kubernetes Tooling and Helpers list | . | sheet, google, wise2c Google sheet with all distributions, including two from wise2c | . | kubecon 2019 Notes from KubeCon and EnvoyCon 2019. | . | . POC . 5pi | jimmy | csdn image mismatch problem | . | .",
            "url": "https://wzhliang.github.io/myfastpage/markdown/2020/03/23/kubernetes-notes.html",
            "relUrl": "/markdown/2020/03/23/kubernetes-notes.html",
            "date": " • Mar 23, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "On Agile",
            "content": "General . a lot of people think Agile is mainly about project management, but the creators of Agile put equal if not greater emphasis on engineering as well. | the software craftsmanship movement preserves the coupling between practices and culture by emphasizing that technical practices are important for agile to be successful. | For any behavior change, people learn first by practicing, and then learn to relate their practices back to principles and values. | Agile is about doing the most valuable thing at any given moment. | Produce running, tested, working, integrated software every two weeks, every week. Build your skills until you can create a new fully operational version every day, twice a day, multiple times a day. | Agile is a set of values and principles generalized from the practices of several similar methodologies. Agile does not define any particular practices; but it has no meaning without them. Agile is the abstract class. Practices are the implementation. Bob Martin on twitter | . | PBI: product backlog items | . Extreme Programming . invented by Kent Beck | prior to scrum | dzone scrum and xp | This book shows readers how to use Scrum, and Agile software development process, to quickly and seamlessly implement XP in their shops while still producing actual software. Using Scrum in the Agile process can virtually eliminate all downtime during an XP implementation. | . | . 12 Practices . The Planning Game | Small Releases | Metaphore | Simple Design | Testing | Refactoring | Pair Programming | Collective Ownership | Continuous Integration | 40-Hour Workweek | On-site Customer | Coding Standards | . Code Smells . &lt;img src=”http://borlandc.pek3b.qingstor.com/soft-skill/code-smells.png” width=75%&gt; | . Scrum . &lt;img src=”http://borlandc.pek3b.qingstor.com/devops/scrum.png” width=65%&gt; . Terms . burndown - how much work is remaining to be done | burnup - how much work has been completed | DRE - defect removal efficiency DRE is the percentage of bugs identified and corrected internally compared to the total bugs in the complete release life cycle. | . | CSAT - customer satisfaction | . User Story Map . &lt;img src=”http://borlandc.pek3b.qingstor.com/devops/user-story-mapping-3.png” width=75%&gt; . User story mapping is a technique developed by Jeff Patton during his long practice as an Agile product owner/scrum master. A story map received its name because it helps map out user stories and other backlog items visually. Items are arranged in two dimensions: The vertical one denotes priority, while the horizontal one represents steps a user takes to perform actions in the system (user journey). . Code Review . dzone 4 types of code review | suggests async by default | . | smartbear general | performance | security | documentation | testing | . | atlassian | dev.to practical principles of code review | . | nyu effective code review check list | . | sonar-go replaces community plugin since May 2018 | . | awesome code review | jboss slides about git but has 2 pages on code review | . | tips, 8 Latency is Key | . | . Goals . goal catch bug | share knowledge | increase readbility | increase communication | learn, mentorship | . | non goal i am smarter than you | i write better code | you suck! | . | . Best Practices . self review by author him/her self | patch size matters 400 lines or less, otherwise breaks into smaller commits | . | time matters 60 minutes or less | . | think of all scenarios | golang: separate vendor commit and actual change | . What to Review . logic error | correct and full understanding of requirements | design: easy to implement, easy to understand ask questions, discuss them! | . | test: is there unit test that can be added? | style: does the code conform to existing style var name | comments adequte | function complexity? | . | unchecked return value | are all input checked? | . User Story . User Story is a small (actually, the smallest) piece of work that represents some value to an end user and can be delivered during a sprint. | template “As a [persona], I [want to], [so that].” | . | Epics are large work items broken down into a set of stories, and multiple epics comprise an initiative. | Epics are not testable, otherwise, user stories should be testable with the exception of non-funcitonal stories | . | INVEST independent | negotiable | valuable | estimable | small | testable | . | atlassian apparently Atlassian knows something about user stories | . | stormotion nice introduction article | . | . Scaling Agile . LeSS - large scale scrum Sprint Planning, Sprint Review and Sprint Retrospective runs at the same time for all teams. | . | . Culture . Team Culture . &lt;img src=”http://borlandc.pek3b.qingstor.com/devops/team-culture.jpeg” width=75%&gt; . by a guy from Windows NT team | referenced by Dave Chaney here | . Links . infoq Ron Jeffries says developers should abandon Agile | Faux Agile, Dark Agile | . | ronjeffries developers abandon Agile | . | the manifestor | wikipedia | cleancoder agile vs manager | . | infoq multiple teams scrum framework | . | refactoring 2nd ed slides on refactoring 2nd edition | by one of the book reviewer | . | martin, 2018, aus the state of agile in 2018 | a talk Martin Fowler given in Austrilia | summary: Get rid of the Agile Industrial Complex and the idea of imposing stuff on teams. Let teams work out the way they should work themselves. | Raise the importance of technical excellence, and never forget that when writing software, the technology side is really vital | and organize around products. | | . | infoq, metrics The Importance of Metrics to Agile Teams | Firstly, the reporting found within common workflow management tools like Jira is not optimised to provide the level of reporting that many teams require for an effective SI programme. | . | .",
            "url": "https://wzhliang.github.io/myfastpage/markdown/2020/03/03/agile-notes.html",
            "relUrl": "/markdown/2020/03/03/agile-notes.html",
            "date": " • Mar 3, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # Title &gt; Awesome summary - toc: true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://wzhliang.github.io/myfastpage/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://wzhliang.github.io/myfastpage/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://wzhliang.github.io/myfastpage/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}